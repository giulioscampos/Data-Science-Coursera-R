qplot(CompressiveStrength, data=training)
plot(training$CompressiveStrength, pch=19, colours=c("blue"))
plot(training$CompressiveStrength, pch=19
plot(training$CompressiveStrength, pch=19
featurePlot(x=training[, c("Cement", "Water", "FlyAsh", "Age")], y=training$CompressiveStrength, plot="pairs")
training
head(training)
plot(training$CompressiveStrength, pch=19)
featurePlot(x=training[, c("Cement", "Water", "FlyAsh", "Age")], y=training$CompressiveStrength, plot="pairs")
training$Age
training$Age>200
qplot(CompressiveStrength, data=training)
plot(CompressiveStrength, data=training)
plot(mixtures$CompressiveStrength)
plot(mixtures$CompressiveStrength, col="age")
plot(mixtures$CompressiveStrength, col="blue")
plot(mixtures$CompressiveStrength, col=mixtures$Age)
plot(mixtures$CompressiveStrength, col=mixtures$FlyAsh)
plot(mixtures$CompressiveStrength, col=mixtures$FlyAsh)
plot(mixtures$CompressiveStrength, col=mixtures$FlyAsh*2)
plot(mixtures$CompressiveStrength, col=mixtures$FlyAsh*1000)
plot(mixtures$CompressiveStrength, col=mixtures$FlyAsh*100000)
plot(mixtures$CompressiveStrength, col=mixtures$FlyAsh*10000000000000000)
plot(mixtures$CompressiveStrength, col=mixtures$FlyAsh*100000000000)
plot(mixtures$CompressiveStrength, col=mixtures$FlyAsh*100000)
plot(mixtures$CompressiveStrength, col=mixtures$FlyAsh*1000000)
plot(mixtures$CompressiveStrength, col=mixtures$FlyAsh*10000000)
plot(mixtures$CompressiveStrength, col=mixtures$Age)
plot(mixtures$CompressiveStrength, col=mixtures$Cement)
plot(mixtures$CompressiveStrength, col=mixtures$BlastFurnaceSlag)
plot(mixtures$CompressiveStrength, col=mixtures$BlastFurnaceSlag*1+1)
plot(mixtures$CompressiveStrength)
plot(mixtures$CompressiveStrength, col=mixtures$Age)
plot(mixtures$CompressiveStrength, col=mixtures$BlastFurnaceSlag*1+1)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training$SuperPlasticizer, main="", xlab="super plasticizer")
training$SuperPlasticizer
training$Superplasticizer
hist(training$Superplasticizer, main="", xlab="super plasticizer")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433);data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433);data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]];training = adData[ inTrain,]
testing = adData[-inTrain,]
training[c("IL_6"),]
training[,c("IL_6")]
tr<-training[,grepl( "IL_" , names( training ) ) ]
tr
head(tr)
tr<-training[,grepl( "IL_" , names( training ) ) ][, -c(TRAIL_R3)]
tr<-training[,grepl( "IL_" , names( training ) ) ][, -c("TRAIL_R3")]
tr<-training[,grepl( "IL_" , names( training ) ) ]
tr[, -c("TRAIL_R3")]
tr[, c("TRAIL_R3")]
tr[, c(-"TRAIL_R3")]
tr[, -c("TRAIL_R3")]
tr[c("TRAIL_R3"),]
tr<-training[,grepl( "IL_" , names( training ) ) ]
tr
head(tr)
tr[, !c("TRAIL_R3")]
tr <- training[,grepl( "IL_" , names( training ) ) ]
tr <- tr[, !names(tr) %in% c("TRAIL_R3")]
head(tr)
columns <- grepl( "IL_" , names( training ) )
ccolumns
columns
columns <- grepl( "IL_" , names( training ) ) or grepl( "diagnosis" , names( training ) )
columns <- grepl( "IL_" , names( training ) ) + grepl( "diagnosis" , names( training ) )
columns
columns <- grepl( "IL_" , names( training ) ) * grepl( "diagnosis" , names( training ) )
columns
columns <- grepl( "IL_" , names( training ) ) + grepl( "diagnosis" , names( training ) )
tr<-training[, columns]
tr
head(tr)
columns <- grepl( "IL_" , names( training ) ) + grepl( "diagnosis" , names( training ) )
columns
training[, columns]
head(training[, columns])
columns <- data.frame(training[,grepl( "IL_" , names( training ) )])
head(columns)
columns <- data.frame(training[,grepl( "IL_" , names( training ) ), 0])
head(columns)
predName <- names(training)
(ILpredictor <- predName[substr(predName, 1, 2) == "IL"])
ProcPCA <- preProcess(training[, ILpredictor], method = "pca", thresh = .9)
ProcPCA$numComp
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433);data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]];training = adData[ inTrain,]
testing = adData[-inTrain,]
trainingIL <- training[, c(ILpredictor, "diagnosis")]
testingIL <- testing[, c(ILpredictor, "diagnosis")]
ModelAll <- train(diagnosis ~ ., data = trainingIL, method = "glm")
confusionMatrix(testingIL$diagnosis, predict(ModelAll, testingIL))
preProc <- preProcess(training[, ILpredictor], method = "pca", thresh = .8)
trainPC <- predict(preProc, training[, ILpredictor])
ModelPCA <- train(trainingIL$diagnosis ~ ., method = "glm", data = trainPC)
testPC <- predict(preProc, testing[, ILpredictor])
confusionMatrix(testingIL$diagnosis, predict(ModelPCA, testPC))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
predName <- names(training)
(ILpredictor <- predName[substr(predName, 1, 2) == "IL"])
ProcPCA <- preProcess(training[, ILpredictor], method = "pca", thresh = .8)
ProcPCA$numComp
data(iris);
library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]; testing <- iris[-inTrain,]
library(carot)
library(caret)
inTrain <- createPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]; testing <- iris[-inTrain,]
library('kernlab');
inTrain <- createPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]; testing <- iris[-inTrain,]
library('caret')
library('caret')
library('ISLR')
inTrain <- createPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]; testing <- iris[-inTrain,]
inTrain <- createDaaPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]; testing <- iris[-inTrain,]
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]; testing <- iris[-inTrain,]
dim(training);dim(testing)
ggplot(Petal.Width, Sepal.Width, colour=Species, data=trainings)
ggplot(Petal.Width, Sepal.Width, colour=Species, data=training)
ggplot(Petal.Width, Sepal.Width, colour=Species, data=training)
qplot(Petal.Width, Sepal.Width, colour=Species, data=training)
modFit <- train(Species ~ ., method="rpart", data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
library(rattle)
fancyRpartPlot(modFit$finalModel)
library(rattle)
fancyRpartPlot(modFit$finalModel)
predict(modFit, newdata=testing)
library(caret)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages('rpart')
install.packages("rpart")
install.packages("rpart")
install.packages("rpart")
install.packages("rpart")
library(rattle)
fancyRpartPlot(modFit$finalModel)
library(ElemStatLearn)
data(ozone, package="ElemStatLearn")
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(ozone, package="ElemStatLearn")
ozone <- ozone[order(ozone$ozone)]
ozone <- ozone[order(ozone$ozone),]
head(ozone)
ll <- matrix(NA, nrow=10, ncol=155)
for (i in 1:10) {
ss <- sample(1:dim(ozone)[1],replace=T)
ozone0 <- ozone0[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone, data=ozone0, span=0.2)
ll[i,] <- predict(loess0, newdata=data.frame(ozone=1:155))
}
# predict temp as a function of ozone
ll <- matrix(NA, nrow=10, ncol=155)
for (i in 1:10) {
ss <- sample(1:dim(ozone)[1],replace=T)
ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone, data=ozone0, span=0.2)
ll[i,] <- predict(loess0, newdata=data.frame(ozone=1:155))
}
plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
for(i in 1:10){lines(1:155, ll[i,], col="grey", lwd=2)}
for(i in 1:10){lines(1:155, ll[i,], col="grey", lwd=3)}
for(i in 1:10){lines(1:155, ll[i,], col="grey", lwd=1)}
plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
for(i in 1:10){lines(1:155, ll[i,], col="grey", lwd=1)}
plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
lines(1:155, apply(ll, 2, mean), col="red", lwd=2)
library(caret)
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B=10, bagControl=bagControl(fit=ctreeBag$fit,
predict=ctreeBag$pred,
aggregate=ctreeBag$aggregate))
treebag <- bag(predictors, temperature, B=10, bagControl=bagControl(fit=ctreeBag$fit,predict=ctreeBag$pred,aggregate=ctreeBag$aggregate))
install.packages('party')
treebag <- bag(predictors, temperature, B=10, bagControl=bagControl(fit=ctreeBag$fit,predict=ctreeBag$pred,aggregate=ctreeBag$aggregate))
data(iris);
library(ggplot2)
names(iris)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,]; testing <- iris[-inTrain,]
dim(training);dim(testing)
qplot(Petal.Width, Sepal.Width, colour=Species, data=training)
library(caret)
modFit <- train(Species ~ ., method="rf", prox=TRUE)
modFit
modFit <- train(Species ~ ., method="rf")
modFit
modFit <- train(Species ~ ., method="rf", prox=TRUE)
modFit
getTree(modFit$finalModel, k=2)
install.packages("tree")
getTree(modFit$finalModel, k=2)
irisP <- classCenter(training[,c(3, 4)], training$Species, modFit$finalModel$prox)
library('caret')
irisP <- classCenter(training[,c(3, 4)], training$Species, modFit$finalModel$prox)
irisP <- classCenter(training[,c(3, 4)], training$Species, modFit$finalModel$prox)
library(ggplot2)
irisP <- classCenter(training[,c(3, 4)], training$Species, modFit$finalModel$prox)
qplot(Petal.Width, Petal.Length, colour=predRight, data=testing, main="newdata Predictions")
pred <- predict(modFit, testing); testing$predRight <- pred==testing$Species
table(pred, testing$Species)
qplot(Petal.Width, Petal.Length, colour=predRight, data=testing, main="newdata Predictions")
# Boosting
library('ISLR')
library('ggplot2')
data(Wage)
summary(Wage)
Wage <- subset(Wage, select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training);dim(testing)
library('caret')
data(Wage)
summary(Wage)
Wage <- subset(Wage, select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training);dim(testing)
modFit <- train(wage, ., method="gbm", data=training, verbose=FALSE)
print(modFit)
qplot(predict(modFit, testing), wage, data=training)
librar('rpart')
# boosting with trees
modFit <- train(wage, ., method="gbm", data=training, verbose=FALSE)
print(modFit)
qplot(predict(modFit, testing), wage, data=training)
library('ISLR')
library('ggplot2')
library('caret')
data(Wage)
summary(Wage)
Wage <- subset(Wage, select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training);dim(testing)
# boosting with trees
modFit <- train(wage, ., method="gbm", data=training, verbose=FALSE)
print(modFit)
qplot(predict(modFit, testing), wage, data=training)
library('ISLR')
library('ggplot2')
library('caret')
data(Wage)
summary(Wage)
wage <- subset(Wage, select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training);dim(testing)
# boosting with trees
modFit <- train(wage, ., method="gbm", data=training, verbose=FALSE)
print(modFit)
qplot(predict(modFit, testing), wage, data=training)
modFit <- train(wage ~ ., method="gbm", data=training, verbose=FALSE)
modFit <- train(wage ~ ., method="gbm", data=training, verbose=FALSE)
install.packages('caret')
install.packages("caret")
modFit <- train(wage ~ ., method="gbm", data=training, verbose=FALSE)
print(modFit)
qplot(predict(modFit, testing), wage, data=training)
# Boosting
library('ISLR')
library('ggplot2')
library('caret')
data(Wage)
summary(Wage)
wage <- subset(Wage, select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training);dim(testing)
# boosting with trees
modFit <- train(wage ~ ., method="gbm", data=training, verbose=FALSE)
qplot(predict(modFit, testing), wage, data=training)
modFit <- train(wage ~ ., method="gbm", data=training, verbose=FALSE)
print(modFit)
qplot(predict(modFit, testing), wage, data=training)
qplot(predict(modFit, testing), wage, data=testing)
library('ISLR')
library('ggplot2')
library('caret')
data(Wage)
summary(Wage)
Wage <- subset(Wage, select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training);dim(testing)
# boosting with trees
modFit <- train(wage ~ ., method="gbm", data=training, verbose=FALSE)
print(modFit)
qplot(predict(modFit, testing), wage, data=training)
qplot(predict(modFit, testing), wage, data=testing)
data(iris)
library(ggplot2)
names(iris)
table(iris$Species)
names(iris)
inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
training <- iris[inTrain,];testing <- iris[-inTrain,]
dim(training);dim(testing)
modlda = train(Species ~ ., data=training, method="lda")
modnb = train(Species ~ ., data=training, method="nb")
modlda = train(Species ~ ., data=training, method="lda")
modnb = train(Species ~ ., data=training, method="nb")
plda = predict(modlda, testing); pnb = predict(modnb, testing)
table(plda, pnb)
install.packages('AppliedPredictiveModeling')
install.packages('ElemStatLearn')
install.packages('pgmm')
install.packages('rpart')
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.7, list = FALSE)
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.7, list = FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Case ~ ., method="rpart", data=training)
modFit$finalModel
plot(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
segmentationOriginal$Case
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
print(modFit$finalModel)
segmentationOriginal$KurtIntenCh1
segmentationOriginal$Case
names(segmentationOriginal)
table(segmentationOriginal$TotalIntenCh4)
predict(modFit, newdata=testing)
inTrain <- data$Case == "Train"
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Case ~ ., method="rpart", data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
modFit <- train(Class ~ ., method="rpart", data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages('rpart')
install.packages('rpart.plot')
library(rattle)
fancyRpartPlot(modFit$finalModel)
set.seed(125)
modFit <- train(Class ~ ., method="rpart", data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
newdata
?t
colMeans(olive)
newdata
treeModel <- train(Area ~ ., data=olive, method="rpart2")
treeModel
predict(modFit, newdata=newdata)
predict(treeModel, newdata=newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
?SAheart
modFit <- train(chd ~ age alcohol obesity tobacco typea ldl, method="rpart", data=trainSA)
modFit <- train(chd ~ age, alcohol, obesity, tobacco, typea, ldl, method="rpart", data=trainSA)
modFit <- train(chd ~ c(age, alcohol, obesity, tobacco, typea, ldl), method="rpart", data=trainSA)
modFit <- train(chd ~ age alcohol obesity tobacco typea ldl, method="rpart", data=trainSA)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=trainSA, family="binamial")
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=trainSA, family="binomial")
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=trainSA, family="binomial")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", data=trainSA, family="binomial")
modFit$finalModel
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
predict(modFit$finalModel, testSA)
sapply(missClass, predict(modFit$finalModel, testSA))
sapply(predict(modFit$finalModel, testSA), missClass)
logitModel <- train(chd ~ age + alcohol + obesity + tobacco +
typea + ldl, data=trainSA, method="glm",
family="binomial")
logitModel
missClass <- function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
predictTrain <- predict(logitModel, trainSA)
predictTest <- predict(logitModel, testSA)
missClass(trainSA$chd, predictTrain) # 0.2727273
missClass(testSA$chd, predictTest) # 0.3116883
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.train)
head(vowel.test)
dim(vowel.train) # 528  11
dim(vowel.test) # 462  11
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
install.packages('randomForest')
install.packages('randomForest')
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
library('randomForest')
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
order(varImp(modelRf), decreasing=T)
install.packages('AppliedPredictiveModeling')
install.packages('caret')
install.packages('ElemStatLearn')
install.packages('pgmm')
install.packages("caret")
install.packages("caret")
install.packages("caret")
library('AppliedPredictiveModeling')
library('caret')
library('ElemStatLearn')
library('pgmm')
library('rpart')
library('gbm')
library('lubritate')
library('forecast')
library('e1071')
install.packages("lubritate")
install.packages("forecast")
install.packages("1071")
install.packages("e1071")
install.packages("e1071")
library('lubritate')
library('forecast')
library('e1071')
install.packages("lubritate")
R.version
if(!require(installr)) {
install.packages("installr"); require(installr)}
updateR()
R.version
